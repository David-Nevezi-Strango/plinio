- Al momento i layer tipo linear-conv1d-conv2d hanno la
  quantizzazione come era in multi-prec-nas ed edmips,
  cioe' (relu + layer), penso sia piu' pulito transire
  verso una formulazione dove il layer quantizza solo pesi
  e bias e le attivazioni sono quantizzate nel layer relu
  che si trovera' successivamente. Questo pero' rende piu'
  difficile calcolare la regolarizzazione quanto dipende
  sia da precisione pesi che precisione attivazioni.
  Forse per la ricerca conviene mantenere il modello edmips?
  Quindi togliendo le relu e basta in fase di conversione.
  Se cosi' fosse in fase di export bisognera' aggiungerle
  nuovamente.

- TODO: Passo di rimozione relu. Va tolta ovunque?

- TODO: Aggiungere MixPrecPool (?)

- TODO: Fixare temperatura.